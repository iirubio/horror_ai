{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Create your own horror movie!!\n"," ### Create a automatically generated horror movie screenplay,  look for movies from the public domain, and automatically let this script edit a new film."]},{"cell_type":"markdown","metadata":{},"source":[" ## Pre-production and screenplay  writing"]},{"cell_type":"markdown","metadata":{},"source":[" ### Let's start by web scraping horror movie screenplays"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import requests\n","import urllib.request\n","import os\n","from bs4 import BeautifulSoup\n","\n","SCREENPLAYS_URLS = ['https://www.simplyscripts.com/genre/horror-scripts.html',\n","                    'https://www.simplyscripts.com/genre/drama-scripts.html',\n","                    'https://www.simplyscripts.com/genre/film-noir-scripts.html',\n","                    'https://www.simplyscripts.com/genre/musical-scripts.html']\n","FILE_TYPES = ['.pdf', '.txt']\n","\n","# Folder with screenplays\n","SCREENPLAYS_FOLDER = 'screenplays'\n","if not os.path.exists(SCREENPLAYS_FOLDER):\n","    os.mkdir(SCREENPLAYS_FOLDER)\n","\n","def scrape_screenplays(urls=SCREENPLAYS_URLS, screenplays_folder=SCREENPLAYS_FOLDER):\n","    \"\"\"Saves screenplays into .txt and .pdf files and returns file names\n","\n","    Keyword Arguments:\n","        urls {string} -- [Urls to scrap] (default: {SCREENPLAYS_URLS})\n","        screenplays_folder {string} -- [Path to files] (default: {SCREENPLAYS_FOLDER})\n","\n","    Returns:\n","        [list] -- [List of file names]\n","    \"\"\"\n","    file_names = []\n","    movie_count = 0\n","    for url in urls:\n","        page = requests.get(url)\n","        soup = BeautifulSoup(page.content, 'html.parser')\n","        for link in soup.find_all('a'):\n","            file_link = link.get('href')\n","            try:\n","                if any(extension in file_link for extension in FILE_TYPES):\n","                    file = requests.get(file_link)\n","                    if '<html>' not in file:\n","                        file_name = file_link.split('/')[-1]\n","                        file_names.append(file_name)\n","                        with open(screenplays_folder + '//' + file_name, 'wb') as f:\n","                            f.write(file.content)\n","                            movie_count += 1\n","                            print(str(movie_count))\n","            except Exception as identifier:\n","                print(identifier)\n","    return file_names\n","\n","file_names = scrape_screenplays()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### We continue to parse the names and get more info about the movies using IMDB's API\n"," ### Preliminary inspection shows a pattern '%20' that should be replaced with a blank space"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from imdb import IMDb # documentation on https://imdbpy.github.io/\n","\n","imdb_object = IMDb()\n","\n","\n","files_data = []\n","for file_name in file_names:\n","    movie_name = file_name[:-4].replace('%20', '') # Erase extension and %20 characters\n","    print(movie_name)\n","    movie_list = imdb_object.search_movie(movie_name)\n","    title = movie_name\n","    rating = None\n","    year = None #TODO: TURN INTO INT\n","\n","    if movie_list:\n","        movie_id = movie_list[0].getID() # Assumes first movie on search is the correct one\n","        movie = imdb_object.get_movie(movie_id)\n","        # TODO: REFACTOR\n","        title = movie['title'] if movie.has_key('title') else movie_name\n","        rating = movie['rating'] if movie.has_key('rating') else None\n","        year = movie['year'] if movie.has_key('year') else None\n","\n","    files_data.append({'file_name': file_name,\n","                        'title': title,\n","                        'rating': rating,\n","                        'year': year\n","                        })\n","\n","# # # Save dataframe to csv\n","# # movies_df = pd.DataFrame(files_data)\n","# # movies_df.to_csv('screenplays_extra_data.csv') #TODO: Refactor screenplays\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### We filter further our new dataset (and do visualizations)\n"," ### We found a significant negative relation between year of publication and rating\n"," ### Reasons for it are not clear given the small dataset, it could be that only \"bad\"\n"," ### recent horror movies have their scripts online, unlike older movies."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Read screenplay files from local folder\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","movies_df = pd.read_csv('screenplays_extra_data.csv')\n","movies_df = movies_df.dropna() # Drop if any row value is NaN\n","# #%%\n","# # Ratings\n","plt.xlim(1, 10)\n","plt.xlabel('Rating')\n","plt.ylabel('Freq')\n","plt.title('Ratings histogram')\n","movies_df['rating'].hist()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Year\n","plt.xlabel('Year')\n","plt.ylabel('Freq')\n","plt.title('Year histogram')\n","movies_df['year'].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Relationship between year and rating (in my small dataset of films with script)\n","import scipy.stats\n","ax = sns.regplot(x='year', y='rating', data=movies_df)\n","scipy.stats.linregress(movies_df['year'], movies_df['rating'])\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### A recurrent neural network is trained with the screenplays, to generate a new one"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as ts\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","SCREENPLAYS_FOLDER = 'screenplays'\n","\n","file_names = movies_df['file_name']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["html_count = 0\n","no_html_count = 0\n","has_html_column = []\n","is_txt = []\n","for file_name in file_names:\n","\n","    text_has_html = False\n","    text = open(SCREENPLAYS_FOLDER + '//' + file_name, 'rb').readlines()\n","\n","    for line in text:\n","        if '<html>' in str(line):\n","            text_has_html = True\n","            break\n","    is_txt.append(True if 'txt' in file_name else False)\n","    has_html_column.append(text_has_html)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["movies_df['has_html'] = has_html_column\n","movies_df['is_txt'] = is_txt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["movies_df.to_csv('screenplays_extra_data2.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","file_names = movies_df[(~movies_df['has_html']) & (movies_df['is_txt'])]['file_name']\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_file = file_names.iloc[5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Read, then decode for py2 compat.\n","# TODO: TURN IT INTO STRING HERE\n","text = open(SCREENPLAYS_FOLDER + '//' + test_file, 'rb').read().decode('utf-8','ignore').encode(\"utf-8\")\n","# length of text is the number of characters in it\n","print ('Length of text: {} characters'.format(len(text)))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The unique characters in the file\n","vocab = sorted(set(str(text)))\n","print ('{} unique characters'.format(len(vocab)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# RNN\n","# Vectorize text\n","# Creating a mapping from unique characters to indices\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text_as_int = np.array([char2idx[c] for c in str(text)])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","text = str(text).replace('b', '')\n","# The maximum length sentence we want for a single input in characters\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create training examples / targets\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","for item in sequences.take(5):\n","    print(repr(''.join(idx2char[item.numpy()])))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for input_example, target_example in  dataset.take(1):\n","  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n","  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","    print(\"Step {:4d}\".format(i))\n","    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n","    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","dataset\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build the model\n","# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                              batch_input_shape=[batch_size, None]),\n","    tf.keras.layers.GRU(rnn_units,\n","                        return_sequences=True,\n","                        stateful=True,\n","                        recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size)\n","  ])\n","  return model\n","\n","model = build_model(\n","  vocab_size = len(vocab),\n","  embedding_dim=embedding_dim,\n","  rnn_units=rnn_units,\n","  batch_size=BATCH_SIZE)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.summary()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_indices\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n","\n","print()\n","print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.compile(optimizer='adam', loss=loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EPOCHS=10\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate text\n","model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","model.build(tf.TensorShape([1, None]))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_text(model, start_string):\n","  # Evaluation step (generating text using the learned model)\n","\n","  # Number of characters to generate\n","  num_generate = 1000\n","\n","  # Converting our start string to numbers (vectorizing)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  # Low temperatures results in more predictable text.\n","  # Higher temperatures results in more surprising text.\n","  # Experiment to find the best setting.\n","  temperature = 2.0\n","\n","  # Here batch size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # remove the batch dimension\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # using a categorical distribution to predict the character returned by the model\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # We pass the predicted character as the next input to the model\n","      # along with the previous hidden state\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(generate_text(model, start_string=u\"ANDY\"))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}